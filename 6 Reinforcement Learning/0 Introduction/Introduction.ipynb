{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Reinforcement Learning -->\n",
    "    \n",
    "    Reinforcement Learning (RL) is a type of machine learning where\n",
    "    an agent learns how to behave in an environment by performing actions\n",
    "    and receiving rewards or penalties. The goal of the agent is to learn\n",
    "    a policy that maximizes the cumulative reward over time.\n",
    "    \n",
    "    Key Components of Reinforcement Learning -->\n",
    "    \n",
    "    Agent: The learner or decision-maker.\n",
    "    Environment: Everything the agent interacts with.\n",
    "    State (S): A representation of the current situation of the agent in the environment.\n",
    "    Action (A): A set of all possible moves the agent can make.\n",
    "    Reward (R): Feedback from the environment to evaluate an action.\n",
    "    Policy (Ï€): A strategy that the agent uses to determine actions based on the current state.\n",
    "    Value Function (V(s)): Predicts the long-term reward of a state under a policy.\n",
    "    Q-Function (Q(s,a)): Predicts the long-term reward of taking action a in state \n",
    "    s and following the policy thereafter.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Workflow of Reinforcement Learning -->\n",
    "    \n",
    "    The agent observes the current state of the environment.\n",
    "    Based on the policy, it takes an action.\n",
    "    The environment transitions to a new state and provides a reward.\n",
    "    The agent updates its policy based on the reward and the observed transition.\n",
    "    This loop continues until the agent learns an optimal policy.   \n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    In the context of Reinforcement Learning, Upper Confidence Bound (UCB) and\n",
    "    Thompson Sampling (TS) are commonly used for tackling the multi-armed bandit\n",
    "    (MAB) problem. These methods aim to balance exploration (trying out less-tested\n",
    "    actions) and exploitation (choosing actions that seem to perform best based\n",
    "    on current information).\n",
    "    \n",
    "    Multi-Armed Bandit Problem -->\n",
    "    \n",
    "    You are given multiple \"arms\" (choices/actions), each with an unknown\n",
    "    probability distribution of rewards.\n",
    "    The goal is to pull the arms in such a way that maximizes the cumulative\n",
    "    reward over time.\n",
    "    \n",
    "    Upper Confidence Bound (UCB) -->\n",
    "    \n",
    "    UCB is a deterministic approach to address the exploration-exploitation trade-off.\n",
    "    It assumes that the true mean reward of an arm is within a certain confidence\n",
    "    interval based on the number of times the arm has been pulled.\n",
    "    \n",
    "    Workflow:\n",
    "    At each time step, calculate the UCB for all arms.\n",
    "    Select the arm with the highest UCB.\n",
    "    Update the rewards and the pull count for the selected arm.\n",
    "\n",
    "    Advantages:\n",
    "    Guarantees logarithmic regret.\n",
    "    Effective when the distribution of rewards has clear separation.\n",
    "    \n",
    "    Limitations:\n",
    "    Works best with stationary reward distributions.\n",
    "    Computationally expensive for a large number of arms due to repeated calculations.\n",
    "    \n",
    "    Thompson Sampling -->\n",
    "    \n",
    "    Thompson Sampling is a probabilistic approach based on Bayesian inference.\n",
    "    It assumes a prior distribution for each arm's reward probabilities and\n",
    "    updates this posterior based on observed rewards.\n",
    "    \n",
    "    Workflow:\n",
    "    Initialize a prior distribution for each arm\n",
    "    (e.g., Beta distribution for binary rewards).\n",
    "    For each arm, sample from its posterior distribution.\n",
    "    Select the arm with the highest sampled value.\n",
    "    Observe the reward and update the posterior distribution for the selected arm.\n",
    "\n",
    "    Advantages:\n",
    "    Naturally balances exploration and exploitation through random sampling.\n",
    "    Handles non-stationary rewards better than UCB.\n",
    "\n",
    "    Limitations:\n",
    "    Computationally more intensive for complex reward distributions.\n",
    "    Requires well-defined priors for reward distributions.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Feature\t                    UCB\t                                    Thompson Sampling\n",
    "    \n",
    "    Approach\t                Deterministic\t                        Probabilistic\n",
    "    Exploration-Exploitation\tExplicit control via confidence term\tImplicit through sampling\n",
    "    Computational Complexity\tModerate\t                            High (due to sampling)\n",
    "    Assumptions\t                Works well with stationary rewards\t    Handles non-stationary rewards\n",
    "    Ease of Implementation\t    Simple\t                                Slightly complex\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Applications -->\n",
    "    \n",
    "    Both methods are widely used in :\n",
    "\n",
    "    Recommendation Systems (e.g., which ad to show a user).\n",
    "    Clinical Trials (e.g., testing multiple treatments).\n",
    "    Online Platforms (e.g., optimizing website layouts).\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
