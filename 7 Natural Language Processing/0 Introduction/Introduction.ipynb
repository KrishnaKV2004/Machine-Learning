{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Natural Language Processing -->\n",
    "    \n",
    "    Natural Language Processing (NLP) is a branch of artificial intelligence\n",
    "    that focuses on enabling computers to understand, interpret, and respond\n",
    "    to human language in a valuable way. It combines linguistics, computer\n",
    "    science, and machine learning to process and analyze large amounts of\n",
    "    natural language data.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Core Concepts in NLP -->\n",
    "    \n",
    "    Tokenization: Splitting text into words, phrases, or sentences.\n",
    "    Stemming and Lemmatization: Reducing words to their root forms.\n",
    "    Part-of-Speech (POS) Tagging: Assigning grammatical tags to words.\n",
    "    Parsing: Analyzing the grammatical structure of a sentence.\n",
    "    Word Embeddings: Representing words in a vector space\n",
    "    (e.g., Word2Vec, GloVe).\n",
    "    Transformer Models: Deep learning models like BERT, GPT, and T5\n",
    "    for advanced NLP tasks.\n",
    "    \n",
    "    Python Libraries -->\n",
    "    \n",
    "    NLTK: A classic library for basic NLP tasks.\n",
    "    spaCy: For industrial-strength NLP processing.\n",
    "    TextBlob: Simplified NLP processing.\n",
    "    Transformers (Hugging Face): For state-of-the-art models like\n",
    "    BERT and GPT.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Key Applications of NLP -->\n",
    "    \n",
    "    Text Classification:\n",
    "    Categorizing text into predefined categories (e.g., spam detection,\n",
    "    sentiment analysis).\n",
    "    \n",
    "    Machine Translation:\n",
    "    Translating text from one language to another (e.g., Google Translate).\n",
    "    \n",
    "    Sentiment Analysis:\n",
    "    Determining the sentiment behind text (e.g., positive, negative, neutral).\n",
    "\n",
    "    Named Entity Recognition (NER):\n",
    "    Identifying entities like names, dates, and locations in text.\n",
    "    \n",
    "    Speech Recognition:\n",
    "    Converting spoken language into text.\n",
    "    \n",
    "    Question Answering:\n",
    "    Building systems that can answer questions posed in natural language.\n",
    "    \n",
    "    Text Summarization:\n",
    "    Automatically creating a summary of a larger text document.\n",
    "    \n",
    "    Chatbots and Virtual Assistants:\n",
    "    Understanding user input and providing intelligent responses.\n",
    "    \n",
    "    Spell Checking and Grammar Correction:\n",
    "    Detecting and suggesting corrections for text errors\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Bag Of Words -->\n",
    "    \n",
    "    The Bag of Words (BoW) model is one of the simplest and most widely\n",
    "    used techniques in Natural Language Processing for text representation.\n",
    "    It is often used for tasks like text classification, sentiment analysis,\n",
    "    and information retrieval.\n",
    "    \n",
    "    What is Bag of Words ?\n",
    "    \n",
    "    The Bag of Words model represents text as a collection of words,\n",
    "    disregarding grammar and word order but keeping track of the frequency\n",
    "    of each word in the text. The resulting representation is a sparse vector,\n",
    "    where each dimension corresponds to a unique word in the corpus.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    How Bag of Words Works -->\n",
    "    \n",
    "    Text Preprocessing:\n",
    "\n",
    "    Convert text to lowercase (to handle case insensitivity).\n",
    "    Remove punctuation, special characters, and numbers.\n",
    "    Tokenize the text into individual words.\n",
    "    Optionally remove stop words (common words like \"the,\" \"is,\" etc.).\n",
    "    Apply stemming or lemmatization to standardize word forms.\n",
    "\n",
    "    Vocabulary Creation:\n",
    "\n",
    "    Compile a list of all unique words (the vocabulary) across the entire\n",
    "    corpus (collection of documents).\n",
    "\n",
    "    Vector Representation:\n",
    "\n",
    "    For each document, count the frequency of each word in the vocabulary.\n",
    "    Represent the document as a vector where each dimension corresponds to\n",
    "    a word in the vocabulary, and the value is the word's count in the document.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Bag Of Words Example -->\n",
    "    \n",
    "    Input Text:\n",
    "    Document 1: \"The cat sat on the mat.\"\n",
    "    Document 2: \"The dog barked at the cat.\"\n",
    "\n",
    "    Preprocessing:\n",
    "    Remove stop words: \"cat sat mat\" and \"dog barked cat\"\n",
    "\n",
    "    Tokenize: [\"cat\", \"sat\", \"mat\"] and [\"dog\", \"barked\", \"cat\"]\n",
    "\n",
    "    Vocabulary:\n",
    "    [\"barked\", \"cat\", \"dog\", \"mat\", \"sat\"]\n",
    "    \n",
    "    Vector Representation:\n",
    "    Doc1 : [0, 1, 0, 1, 1]\n",
    "    Doc2 : [1, 1, 1, 0, 0]\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#   Bag Of Words -->\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample text corpus\n",
    "documents = [\n",
    "    \"The cat sat on the mat.\",\n",
    "    \"The dog barked at the cat.\"\n",
    "]\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "\n",
    "# Fit and transform the documents\n",
    "X = vectorizer.fit_transform(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary -->  ['barked' 'cat' 'dog' 'mat' 'sat']\n"
     ]
    }
   ],
   "source": [
    "# Display the vocabulary\n",
    "print(\"Vocabulary --> \", vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW Representation -->\n",
      "\n",
      " [[0 1 0 1 1]\n",
      " [1 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Display the vector representation\n",
    "print(\"BoW Representation -->\\n\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Bag of Words is often replaced by more advanced techniques like\n",
    "    Word Embeddings (Word2Vec, GloVe) and Transformers (BERT, GPT)\n",
    "    for modern NLP tasks, as these models capture context and semantic\n",
    "    meaning better. However, BoW remains a valuable tool for basic NLP\n",
    "    tasks and as a baseline method.\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
