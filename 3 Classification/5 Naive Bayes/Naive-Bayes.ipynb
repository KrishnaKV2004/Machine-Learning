{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Naive Bayes -->\n",
    "    \n",
    "    Naive Bayes is a family of simple yet effective probabilistic classifiers based on applying Bayes'\n",
    "    Theorem with a strong (and often unrealistic) assumption of feature independence.\n",
    "    It's widely used in text classification, spam detection, sentiment analysis, and medical diagnosis\n",
    "    due to its simplicity and efficiency.\n",
    "    \n",
    "    It uses Bayes' Theorem, a formula for calculating conditional probabilities, and assumes that all\n",
    "    features contribute independently to the probability of a class.\n",
    "    \n",
    "    Imagine you're a detective trying to figure out which suspect committed a crime based on evidence\n",
    "    (features). If suspect A has more evidence pointing to them than suspect B, you'll probably accuse\n",
    "    suspect A. Naive Bayes works similarly by weighing the evidence (probabilities of features) for each\n",
    "    class and picking the one with the highest likelihood.\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='Source/Bayes Theorem.png' alt='Bayes Theorem' style=\"width:500px; height:auto; margin-left:40px;\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Types of Naive Bayes -->\n",
    "    \n",
    "    Gaussian Naive Bayes :\n",
    "\n",
    "    Assumes that the data is normally distributed.\n",
    "    Commonly used for continuous data.\n",
    "    \n",
    "    Multinomial Naive Bayes :\n",
    "\n",
    "    Suitable for discrete data like word counts or frequencies.\n",
    "    Popular in text classification tasks.\n",
    "    \n",
    "    Bernoulli Naive Bayes :\n",
    "\n",
    "    Designed for binary data (e.g., presence/absence of a feature).\n",
    "    Often used in binary text classification.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Steps in Naive Bayes Classification -->\n",
    "    \n",
    "    Prepare the Dataset :\n",
    "\n",
    "    Extract features and label them.\n",
    "    Divide the dataset into training and testing sets.\n",
    "\n",
    "    Calculate Probabilities :\n",
    "\n",
    "    Compute the prior probabilities of each class.\n",
    "    Compute the likelihood P(feature|class)) for each feature.\n",
    "    \n",
    "    Apply Bayes' Theorem :\n",
    "\n",
    "    Combine the priors and likelihoods for prediction.\n",
    "\n",
    "    Predict the Class :\n",
    "\n",
    "    Select the class with the highest posterior probability.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "    Advantages -->\n",
    "    \n",
    "    Fast and Efficient: Works well with large datasets.\n",
    "    Simple : Easy to implement and interpret.\n",
    "    Performs Well on Sparse Data: Especially effective in text data.\n",
    "    \n",
    "    Limitations -->\n",
    "    \n",
    "    Feature Independence Assumption: Rarely true in real-world data.\n",
    "    Zero Frequency Problem: If a feature value doesn't exist in the training dataset, it gets\n",
    "    zero probability. (Solution: Laplace Smoothing).\n",
    "    Sensitive to Irrelevant Features: Can be misled if irrelevant features dominate the dataset\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
